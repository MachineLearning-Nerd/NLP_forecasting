Slide 1.
Hello everyone. Let me introduce you to my work called A text-based forecasting model for equity trading.

Slide 2.
The goal of this work is to suggest a flexible framework for incorporating textual features into forecasting models, specifically stock price and stock return forecasting models. This is a quite important problem, as a lot of price sensitive information is contained in a textual form and many fund managers are trying to use natural language processing to better manage their investment portfolios. However, they often restrict their attention to the sentiment analysis task (measuring sentiment with regard to a particular stock in news and social media). I believe this approach is limited, because a lot of price sensitive information may have neutral sentiment or because sentiment might be not supported by cash flow analysis. So, a better approach could be applying the natural language understanding task by training a model to directly predict stock returns from textual inputs.

Slide 3.
I constructed a dataset of significant development headlines for 44 blue-chip stocks in the period from 2010 to 2019 to be used as text-based features. Significant developments is a news analysis and filtering service that identifies important company news on a real-time basis. After preprocessing, the number of headlines in the dataset totaled to aproximately 10,000.
In addition, I collected a dataset of numerical features which included returns of S&P500 index, money flow indicator, forward price-to-earnings ratio and short interest. The purpose of the numerical dataset is to test the model in a close to real-life environment, as well as to construct one of the baseline models.

Slide 4.
I perform preprocessing of the headlines in five steps. The first step includes filtering out those sentences that contain unexpected characters, for example, written in a foreign language. On the second step, headlines I aggregate headlines by release date so that multiple headlines released on the same date are joined into one sentence though a special "new line" token. On the next steps, headlines are tokenized and tagged by part of speech. Finally, I do lemmaizatoin of each token based on the information received on the previous steps.

Slide 5.
The implemented neural architecture is illustrated on this picture. It starts from two embedding layers, which are applied to each token of a tokenized textual feature. The first embedding layer produces word-level embeddings based on a pre-trained Word2Vec model. The second embedding layer is a Char CNN model that learns word embeddings during training from the sequence of chars.
Combined word embeddings then go through a BiLSTM layer. Its hidden states are used to compute attention weights, which determine those tokens that are relatively more important in forecasting. Embeddings produced by BiLSTM layer are then weighted by attention weights and summed up into a context vector.
Finally, the obtained context vector is combined with the numerical features, and this combined feature vector through regression or classification layers, depending on the nature of the forecasted variable.





Slide 9.
That is it. Thank you for watching!
